### 1.探索数据
### 原始特征数据

>原始特征均为均值为0，1的两个正态分布的组合

![](Pasted%20image%2020251106084022.png)

### 多项式扩展特征数据

>经过只保留交互项的多项式扩展后，新特征数据在高阶交互是倾向于归零，只有小于1%的数据是实际有效的，扩展过程产生了大量噪声

![](poly_features_n12_e8_sequential_viz.png)
***

### 2.逻辑回归分析

#### 2.1 收敛性分析

>无论进行何种基扩展，使用二元交叉熵损失函数的对数几率回归一定是存在最优解的（是凸优化问题），所以模型一定可以收敛

梯度公式
$$\theta:=\theta-\alpha·\nabla J(\theta)$$
$$\theta:=\theta-\alpha\frac{1}{m}X^{T}(\sigma(X·\theta)-y)$$
带L2正则化的梯度

$$J(\theta)=[\frac{1}{m}X^{T}(\sigma(X·\theta)-y)]+\frac{\lambda}{m}\theta_{reg}$$

>可以看到公式里有 X，这里就是输入的特征数据，尺寸=样本数 X 特征数，X 的数值会直接影响梯度计算。

#### 2.2 与数据结合分析

**因为多项式基扩展带来的超大量的接近0数据，只有1%有效数据，所以X矩阵内大部分值接近0**

* 这意味着，对于 θj​ 来说，**在 99.9% 的训练时间里，它接收到的梯度信号都是 0**
* 只有在那 0.1% 的、极其罕见的、该特征被“激活”（xij​≈1）的样本上，θj​ 的梯度才**会**有一个“非零”的更新信号。
* **这解释了为什么训练迭代超过100000次也未收敛**

#### 2.3 简单解决方案

**使用标准化**

* 让很小的梯度变大一点，让他也能有意义 (效果不好)

***

### 3.探索机器学习模型（不进行过多特征工程）

### 3.1 多项式基扩展的对数几率回归 （包含数据标准化）

>模型实际仍未收敛，但是已经出现了非常严重的过拟合。
>这种过拟合可能来源于数据中包含了过多的噪声。

``` python
N_ITERATIONS = 50000 # (max iterations)

    logreg = LogisticRegression(
        penalty='l1', 
        max_iter=N_ITERATIONS, 
        solver='qn',         
        C=0.85               
    )
```

![](polynomial_regression_accuracy_unified_plot_GPU.png)


#### 3.2 SVM (RBF)

>SVM在n=12的时候训练集准确率达到100%，出现严重过拟合

![](svm_rbf_accuracy_gridsearch_GPU.png)

#### 3.3 XGBoost

![](xgb_accuracy_gridsearch_GPU.png)
#### 3.4 MLP


***

### 4.探索特征工程

#### 4.1 二值化（尝试降噪）

#### 4.2 映射到-1，1（人工注入知识）

#### 4.3 核化PCA

#### 4.4 