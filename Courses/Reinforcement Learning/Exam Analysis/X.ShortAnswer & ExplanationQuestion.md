根据对过去五年（2020-2025）考试试卷的分析，**需要用文字描述的简答题（Short Answer / Explanation Questions）占比大约在 25% 到 35% 左右**。这类题目通常集中在试卷的 **第 3 题**（或最后一题）以及 **第 2 题的后半部分**。

与计算题和多项选择题不同，这类题目要求考生展示对概念的深度理解，而非单纯的公式推导。

以下是简答题高频出现的章节及具体考察形式：

### **1. 深度强化学习与函数近似 (Lecture 5: Value Function Approximation & Deep RL)**

这是简答题出现频率**最高**的领域，特别是关于 DQN 及其变种的原理。

- **函数近似的必要性：**
    
    - **2024-2025 Q3a:** 要求解释为什么在 RL 中有时首选函数近似（Function Approximation）而不是查找表（Lookup Table） 1。
        
- **DQN 架构与超参数：**
    
    - **2021-2022 Q3a:** 要求画出并比较“原始 DQN”架构与“朴素 DQN”架构的区别，并解释原始架构的潜在优势 2。
        
    - **2021-2022 Q3c:** 讨论 DQN 的超参数（帧数 $k$ 与 Replay Buffer 大小 $L$）之间的关系，要求通过文字辩论“同意或不同意”某个观点 3。
        

### **2. 策略梯度方法 (Lecture 6: Policy Gradient Methods)**

这一章的理论性较强，经常考察算法背后的直觉和数学原理的定性解释。

- **REINFORCE 算法的性质：**
    
    - **2024-2025 Q3c (iii):** 解释 REINFORCE 算法为什么具有高方差（High Variance），并描述两种降低方差的解决方案及其原理 4。
        
    - **2024-2025 Q3c (i):** 解释在更新参数 $\theta$ 时，目的是最大化还是最小化 $J(\theta)$ 及其原因 5。
        
- **策略梯度定理：**
    
    - **2023-2024 Q3a:** 简要解释策略梯度定理在 RL 中使得什么成为可能（即解决无法对环境动力学求导的问题） 6。
        
    - **2021-2022 Q3b:** 解释策略梯度方法解决了什么复杂性问题，以及该定理提供了什么解决方案 7。
        

### **3. 基础算法对比与分析 (Lecture 3 & 4: MC vs TD)**

这类题目要求对比不同算法在特定环境下的表现，通常不涉及复杂计算，而是定性分析。

- **收敛速度与方差分析：**
    
    - **2022-2023 Q2b:** 这是一个经典的定性分析题。要求解释在随机奖励和确定性奖励两种情况下，TD(0) 和 Every-visit Monte Carlo 谁收敛更快以及原因（不需要量化，只需定性论证） 8。
        
- **On-policy vs Off-policy：**
    
    - **2022-2023 Q3b:** 讨论 Off-policy 方法是否总体上对 Model-free Deep RL 比 On-policy 方法更有利，并给出理由 9。
        

### **4. 开放式设计与应用题 (Open-ended Design)**

这类题目偶尔出现，要求考生运用所学知识解决一个全新的场景问题。

- **场景设计：**
    
    - **2020-2021 Q2a (Mechanical Horse Race):** 给定一个赛马博彩场景，要求解释如何找到最佳投注策略。题目明确指出“不需要重述算法细节，但要提供选择该方法的理由，并讨论预测它将如何工作” 10。
        
    - **2020-2021 Q2b (ii):** 通过绘制动力学模型图，解释机器人是否能使用特定方法达到目标 11。
        

---

### **总结：如何应对简答题？**

1. **关注 "Why" 和 "How":** 不要只记公式。复习时要多问自己：为什么 DQN 需要 Target Network？为什么 REINFORCE 方差大？MC 和 TD 的本质区别是什么？
    
2. **关键词得分:** 阅卷通常按点给分。例如提到 DQN 优势时，关键词包括 "Correlations" (相关性), "I.I.D." (独立同分布), "Moving Target" (移动目标) 等。
    
3. **比较思维:** 考试非常喜欢让你比较两个概念（如 Algorithm 1 vs Algorithm 2, On-policy vs Off-policy）。
    
4. **特定章节:** 重点复习 **Lecture 5 (DQN)** 和 **Lecture 6 (Policy Gradient)** 的文字概念部分，这两章在 Q3 的简答题中占比极大。