### 1.探索数据
### 原始特征数据

>原始特征均为均值为0，1的两个正态分布的组合

![](Pasted%20image%2020251106084022.png)

### 多项式扩展特征数据

>经过只保留交互项的多项式扩展后，新特征数据在高阶交互是倾向于归零，只有小于1%的数据是实际有效的，扩展过程产生了大量噪声

![](poly_features_n12_e8_sequential_viz.png)

### 特征的低阶相关性 （使用了二值化的数据）

>证明数据不存在二阶交互关系
>（是否二值化不影响热力图）

![](correlation_heatmap.png)


![](pairplot_subset.png)

### 尝试可视化高阶交互

>证明了最起码需要4阶的交互，即仅使用低阶交互不可行
>（这里面的点红色和蓝色是重叠在一起的，因为二值化加上作图先后顺序可能导致误解）

![](3way_interaction_3D_plot.png)
### 流形分析

>“自动的”、基于**流形/距离**的降维算法（t-SNE 和 UMAP），无法在高维噪声中解开这个高阶信号。它们的可视化结果是 50/50 的随机混合。
>
>所有依赖“局部性”、“距离”、“近邻”或“贪婪搜索”的“自动”模型（SVM, XGBoost, t-SNE, UMAP）都是“**隐形**”的。
>
>因此需要进行特征的重新构造，才能保证这些模型自动学会特征表达

![](manifold_learning_comparison.png)

### 核化 PCA

>依然失效，提前预告了使用RBF核的SVM的失败

![](kpca_rbf_visualization.png)

### 数据分析结果

1. 当前0与1的数据在进行不要x^n的多项式基扩展时，在高阶交互时值会偏向0，导致大量噪声。
2. 但仅使用低阶交互数据基本不可行，需要找到多个特征之间的高阶交互。
3. 目前猜测除了真正位于0和1的点，其他偏离0和1的点会带来噪声。
4. 如果不进行任何显式的特征工程，理论上只有MLP可以达到准确率目标（自动特征工程）
5. 逻辑回归，核化SVM在求解二分类问题时都是凸的，导致他们局限性的主要因素应该来自数据
6. 根据垃圾进垃圾出原理，考虑为每一种可能的模型进行专门的特征工程。
7. 根据流形分析的情况，重新构造数据是有意义的

***

### 2.逻辑回归分析

#### 2.1 收敛性分析

>无论进行何种基扩展，使用二元交叉熵损失函数的对数几率回归一定是存在最优解的（是凸优化问题），所以模型一定可以收敛。
>
>但是这种收敛是收敛到模型在给定训练数据上能做到的最好情况，不一定是整个问题的最优解，同时可能存在严重过拟合。

梯度公式
$$\theta:=\theta-\alpha·\nabla J(\theta)$$
$$\theta:=\theta-\alpha\frac{1}{m}X^{T}(\sigma(X·\theta)-y)$$
带L2正则化的梯度

$$J(\theta)=[\frac{1}{m}X^{T}(\sigma(X·\theta)-y)]+\frac{\lambda}{m}\theta_{reg}$$

>可以看到公式里有 X，这里就是输入的特征数据，尺寸=样本数 X 特征数，X 的数值会直接影响梯度计算。

#### 2.2 与数据结合分析

**因为多项式基扩展带来的超大量的接近0数据，只有1%有效数据，所以X矩阵内大部分值接近0**

* 这意味着，对于 θj​ 来说，**在 99.9% 的训练时间里，它接收到的梯度信号都是 0**
* 只有在那 0.1% 的、极其罕见的、该特征被“激活”（xij​≈1）的样本上，θj​ 的梯度才**会**有一个“非零”的更新信号。
* **这解释了为什么训练迭代超过100000次也未收敛**

#### 2.3 简单解决方案

**使用标准化**

* 让很小的梯度变大一点，让他也能有意义 (效果不好)

***

### 3.探索机器学习模型（不进行过多特征工程）

### 3.1 多项式基扩展的对数几率回归 （包含数据标准化）

>模型实际仍未收敛，但是已经出现了非常严重的过拟合。这种过拟合可能来源于数据中包含了过多的噪声。
>
>由于已经证明模型一定可以收敛，所以应该尝试对数据进行处理。

``` python
N_ITERATIONS = 50000 # (max iterations)

    logreg = LogisticRegression(
        penalty='l1', 
        max_iter=N_ITERATIONS, 
        solver='qn',         
        C=0.85               
    )
```

![](polynomial_regression_accuracy_unified_plot_GPU.png)


#### 3.2 SVM (RBF)

>SVM在n=12的时候训练集准确率达到100%，出现严重过拟合 （数据未进行二值化）
>
>可能原因是出现维度灾难，导致信噪比极低以至于接近0，模型开始 “背诵”

```python
    grid_C_vals = [10.0, 1.0, 0.5]
    grid_gamma_vals = ['auto', 0.5, 0.1]
```

![](svm_rbf_accuracy_gridsearch_GPU.png)

#### 3.3 XGBoost

>XGBoost 在求解二分类问题的时候是非凸的，理论上他甚至不一定可以找到训练集上的最优解
>
>这里存在一个猜测，XGBoost的每一棵树在每次分裂的时候只计算一个属性的信息增益，他可能无法完全考虑到特征间的高阶交互

```python
    grid_max_depth = [3, 5, 8]       # (Shallow trees vs. deeper trees) (浅树 vs. 深树)
    grid_n_estimators = [150]        # (Fixed number of trees for speed) (固定树的数量以提高速度)
    grid_learning_rate = [0.1, 0.05] # (Fast vs. slow learning) (快 vs. 慢 学习)
```

![](xgb_accuracy_gridsearch_GPU.png)
#### 3.4 MLP

>G.O.A.T
>
>自动进行特征工程，不需要对数据进行任何处理即可达到超高准确率

```python
    grid_hidden_layer_sizes = [(300) ]
    grid_alpha = [0.1, 0.01] 
    grid_learning_rate_init = [0.001]
```


![](mlp_accuracy_gridsearch_CPU.png)

<font color="Red">猜测特征量增加时信噪比持续降低，导致模型过拟合</font>

***

### 4.探索特征工程

#### 4.1 二值化（尝试降噪）

**二值化多项式基扩展对数几率回归**

>在n=10,12的时候存在一些提升（尤其是12），在之后又因为信噪比导致过拟合
>
>猜测二值化实现了一定程度的降噪，在降噪这个方向有研究空间

![](polynomial_regression_accuracy_unified_plot_GPU(1).png)

**二值化SVM**

>没有明显提升，过拟合仍然非常严重


![](svm_rbf_accuracy_gridsearch_GPU(1).png)

**二值化XGBoost**

>效果依然很差，不明确如何提升效果


![](xgb_accuracy_gridsearch_GPU(1).png)


**二值化MLP**

>效果进一步提升，基本可以认为二值化是合理的

![](mlp_accuracy_gridsearch_CPU(1).png)

#### 4.2 映射到-1，1（人工注入知识）

**PLR**

>性能巨大提升，满足所有准确率条件
>
>解决了高次交互项为0的问题，极大程度提高信噪比 ？

![](polynomial_regression_accuracy_unified_plot_GPU(2).png)

**SVM** 

>没变化
>
>RBF核进行了隐式特征工程，导致局限性的原因主要还是来自维度灾难

**XGBoost**

>没变化
>
>XGBoost通过分裂进行特征工程，导致局限性的原因可能来源于模型无法考虑高阶交互项

**MLP**

>变化不大
>
>MLP内部自动进行特征工程，对特征进行一次基变换 2x-1 对结果没影响

![](mlp_accuracy_gridsearch_CPU(2).png)


#### 4.3