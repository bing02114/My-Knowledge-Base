### 1.Introduction to Monte Carlo vs. Dynamic Programming

**Dynamic Programming**

>Requires a complete model of the environment. It updates value estimates based on the Bellman equation, using the estimates of other states (a process called bootstrapping) 

**Monte Carlo**

>Are model-free, meaning they do not need to know the environment's dynamics. They learn by generating sample episodes and averging the observed returns. MC methods do not bootstrap; they wait until the end of an episode to update value functions based on the actual, final return

![](./images/DPvsMC.png)

***
### 2.Monte Carlo Policy Evaluation

>The goal of policy evaluation is to estimate the value function $V^{\pi}(s)$ for a given policy $\pi$. MC methods achieve this by:

1. Generate episodes following policy $\pi$
2. For each episode, compute the return Gt from each state s
3. Average all returns observed from state s

$$G^{\pi}(s)≈\frac{1}{n}\sum^{n}_{i=1}G_{i}(s)$$

**Key insight**

>As we collect more episodes, our estimates converge to the true values by the **Law of Large Numbers**

**First-Visit vs. Every-Visit**

>When a state is visited multiple times in one episode, "first-visit MC" only considers the return from the first time the state was visited, while "every-visit MC" averages the returns from all visits

![](Reinforcement%20Learning/images/FirstVisitMC.png)

><font color="REd">The main advantage of MC over DP is: No model required</font>

***
### 3.Monte Carlo Control

>MC Control aims to find the optimal policy. Since a model is not available, we cannot use V(s) to find the best action. Instead, we must estimate the action-value function, Q(s,a)

#### 3.1 The Exploration Problem

>To learn the optimal Q-values, the agent must explore all possible actions from all states. If the policy is deterministic, it will only ever sample one action per state, preventing exploration. This is solved using stochastic (non-deterministic) policies

**Ways to solve the exploration problem**

1. Exploring Starts

>Assuming episodes can start from any state-action pair
>Every (s,a) pair has nonzero probability of being selected as start
>Guarantee all pairs visited infinitely often
>**Unrealistic in real environments**


2. $\epsilon-greedy$ 


**$\epsilon-greedy$ policies**

$$
f(x)=\left\{
\begin{matrix}
1-\epsilon+\frac{\epsilon}{|A(s)|} & {if~a=~\arg\max_{a'}Q(s,a')} \\ \frac{\epsilon}{|A(s)|} & otherwise
\end{matrix}
\right.
$$


<font color="Red">Strategies to introduce of increase exploration in MC control</font>

>1. Exploring Starts
>
>This method assumes that every episode can start from any state-action pair (s,a) with a non-zero probability. By forcing the agent to explore all possible starting conditions, it guarantees that all state-action pairs are visited infinitely often in the limit. However, this is often unrealistic for real-world environments
>
>2. ϵ-Greedy Policies (On-policy)
>
>This is an on-policy method where the agent follows a "soft" policy. With probability 1−ϵ, the agent selects the greedy action (the best estimated action), and with probability ϵ, it selects an action uniformly at random from all available actions. This ensures that all actions have a non-zero probability of being selected, maintaining exploration throughout learning.
>
>3. Off-Policy Learning
>
>This approach uses two separate policies: a **target policy** π (usually deterministic/greedy) that is being learned, and a **behavior policy** b that is used to generate experience. The behavior policy is designed to be more stochastic (e.g., using a high ϵ) to ensure sufficient exploration of the state-action space, while the target policy exploits the learned knowledge.


#### 3.2 On-policy Methods

• Evaluate and improve the policy that is used to make decisions
• The policy being learned is the same as the policy being followed
• Must use stochastic policies to maintain exploration

![](../Reinforcement%20Learning/images/onpolicyMC.png)

><font color="REd">This method finds an optimal _near-optimal_ policy, not the true optimal one, because it must always maintain some level of exploration.</font>

![](../Reinforcement%20Learning/images/epsilongreedyproof.png)
#### 3.3 Off-policy Methods

• Learn about one policy while following a different policy
• Target policy: what we want to learn about
• Behavior policy: what we use to generate experience

**The Exploration-Exploitation Dilemma**

• Want to learn about the optimal policy (deterministic, greedy)
• Need to explore to find all good actions (stochastic, random)
• Can’t do both with the same policy!

**Off-policy Solution**

• Target policy π: The policy we want to evaluate/improve (can be
deterministic)
• Behavior policy b: The policy we use to generate data (must be
stochastic)
• Learn about π from data generated by b

>Every action taken by $\pi$ must also be takeable by b 

**Importance Sampling**


![](../Reinforcement%20Learning/images/offpolicyMC.png)

><font color="Red">This method allows learning the true optimal policy while still exploring</font>

***
### 4.Limitations of Monte Carol Methods

* They only work for **episodic tasks** (tasks with a clear end)
* Learning only occurs at the end of each episode, which can be slow.
* The returns can have high variance, potentially requiring many episodes to converge
