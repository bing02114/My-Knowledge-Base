### **1. 手动计算 MC 估计值 (Manual Calculation of MC Estimates)**

**考察内容：**

- **核心区别：** 考试极其喜欢考察 **First-visit (首次访问)** 与 **Every-visit (每次访问)** 蒙特卡洛方法的区别。
    
- **计算逻辑：** 给定一条具体的轨迹（Trace），例如 $S_1 \to S_2 \to S_1 \to S_2 \to S_T$，要求计算特定状态的价值估计。
    
    - **First-visit:** 对于状态 $s$，只计算它在该轨迹中**第一次**出现时的回报 $G_t$。
        
    - **Every-visit:** 对于状态 $s$，每次出现都要计算对应的回报，并求平均。
        

**考察形式：**

- **计算大题 (Trace Analysis):**
    
    - **2024-2025 Q2a:** 给定轨迹 $(s_1, s_2, s_1, s_2, s_T)$ 和确定性奖励，分别计算 $s_1$ 和 $s_2$ 的 First-visit 和 Every-visit MC 回报估计值 1。
        
    - **2023-2024 Q2a:** 同样给定包含循环的轨迹 $S_2, S_2, S_1, S_2, S_3$，要求计算 First-visit MC 下各状态的价值 2。
        

### **2. MC 与 TD 的收敛性及性质对比 (MC vs. TD Comparison)**

**考察内容：**

- **收敛速度与方差：** MC 无偏但方差大（受随机奖励和长轨迹影响），TD 有偏但方差小。
    
- **环境因素：** 考察在**确定性环境**与**随机环境**（如随机奖励）下，MC 和 TD 谁收敛更快。
    
- **自举 (Bootstrapping):** MC 不使用自举（基于真实回报 $G_t$），而 DP 和 TD 使用自举。
    

**考察形式：**

- **简答/分析题 (Qualitative Argument):**
    
    - **2022-2023 Q2b:** 这是一个高频难点。题目设置了两种情况（奖励是随机的 Bernoulli 分布 vs 奖励是确定性的），要求解释 TD(0) 和 Every-visit MC 谁收敛更快及原因。
        
        - _关键点：_ MC 受回报随机性影响大（高方差），而 TD 利用贝尔曼误差，受方差影响较小，通常初期收敛更快 3。
            
- **多项选择题 (MCQ):**
    
    - 判断关于 MC 采样、自举和收敛条件的陈述是否正确 4。
        

### **3. 探索与控制策略 (Exploration & Control)**

**考察内容：**

- **探索策略：** 如何在无模型控制中保证所有状态都能被访问？
    
    - **Exploring Starts (探索性出发):** 假设从所有状态-动作对开始都有非零概率 5。
        
    - **$\epsilon$-greedy:** 以 $\epsilon$ 概率随机选择动作，保证探索 6。
        
- **On-policy vs Off-policy:** 区分同策略（评估和改进的是同一个策略）与异策略（行为策略与目标策略不同）。
    

**考察形式：**

- **简答题 (Sketch Strategies):**
    
    - **2022-2023 Q2a:** 问“如何在基于蒙特卡洛的控制中引入或增加探索？简要描述几种策略。”（答案点：$\epsilon$-greedy, Exploring Starts, 乐观初始值等）7。
        
- **伪代码分析:**
    
    - **2023-2024 Q1g:** 给出一段算法伪代码，问它是 On-policy 还是 Off-policy，以及是否使用了 Soft policy 8。
        

### **4. 重要性采样 (Importance Sampling)**

**考察内容：**

- **Off-policy 基础：** 理解为什么 Off-policy 需要重要性采样（纠正数据分布偏差）。
    
- **普通 vs 加权 (Ordinary vs Weighted):**
    
    - 普通重要性采样：无偏，但方差可能无穷大 9。
        
    - 加权重要性采样：有偏，但方差较小 10。
        

**考察形式：**

- **多项选择题:** 考察 Off-policy 方法的定义、覆盖假设 (Coverage Assumption) 以及重要性采样的作用。
    
- **注意：** 虽然 Lecture 3 花了大量篇幅讲解重要性采样的数学公式，但在近五年的大题计算中，直接要求手动计算复杂 IS 权重的题目较少，更多侧重于概念理解或简单的 Off-policy 算法识别。

### **备考建议 (针对 Lecture 3)**

1. **必拿分项：** 务必熟练掌握 **First-visit MC** 的计算。遇到轨迹中有重复状态（Loop）的情况，一定要小心只取第一次出现的时刻计算 Gt​。
    
2. **理解核心差异：** 深入理解 MC 和 TD 在“方差 vs 偏差”以及“对随机奖励敏感度”上的区别。22/23 年的 Q2b 是这一考点的典型代表。
    
3. **关注术语：** 考试中可能会给出一段从未见过的伪代码，要求你判断它是 On-policy 还是 Off-policy。判断依据是：生成数据的策略（Behavior Policy）和更新目标的策略（Target Policy/由 max Q 决定的策略）是否一致。
    
4. **公式记忆：** 记住回报 Gt​ 的递归计算公式 Gt​=Rt+1​+γGt+1​，这在手动计算 Trace 时非常有用。













***
***
OLD VERSION
### 考点1 回报计算

**计算**

* 会计算首次和每次访问蒙特卡洛方法的回报（计算题）

### 考点2 MC 种类

**伪代码判断**

* 从伪代码中判断模型是MC还是TD，或者是哪一种MC

### 考点3 $\epsilon$-greedy 策略

**定义**

* $\epsilon$ 对动作选择概率的影响

**性质**

* soft-policy, 保证强化学习的探索
* 与策略提升定理的关系

### 考点4 随机起点

* 在实际与环境交互时是否可以采用随机起点、在哪一步应用

### 考点5 on-policy 与 off-policy

* on-policy持续探索的理解