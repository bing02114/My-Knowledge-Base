
#### **1. 理论概念理解 (Theoretical Understanding)**

**考察形式：** 多项选择题 (MCQ) **考察频率：** 高 (2022, 2024)

考试经常通过选择题的形式，考察对 MDP 基本定义和性质的理解。你需要能够辨析关于状态、动作、奖励和动态特性的陈述是否正确。

- **马尔可夫性质与动态特性 (Dynamics):**
    
    - 考察环境动力学 p(s′,r∣s,a) 是否完全表征了环境。
        
    - 考察未来的状态和奖励是否仅依赖于当前状态和动作，而与历史无关（马尔可夫性质）。
        
- **最优策略与价值函数 (Optimality):**
    
    - **折扣因子 (γ):** 考察 γ∈[0,1] 对价值函数有限性的影响。
        
    - **唯一性:** 考察最优策略是否唯一（通常不唯一），以及最优价值函数是否唯一（通常是唯一的）。
        
    - **贝尔曼最优方程:** 识别 v∗ 和 q∗ 的正确数学表达形式（包含 `max` 操作符）。


#### **2. 贝尔曼方程的求解与计算 (Solving Bellman Equations)**

**考察形式：** 计算题 / 简答题 **考察频率：** 非常高 (2021, 2022, 2024)

这类题目通常给出一个包含 2-3 个状态的小型 MDP 或 MRP（马尔可夫奖励过程），要求你利用贝尔曼期望方程列出线性方程组，手动计算出特定状态的价值 V(s) 或动作价值 Q(s,a)。

- **特定场景计算:**
    
    - **天气 MRP (2021):** 给定“晴天”和“雨天”的状态转移图，要求计算 γ=0.9 时的状态价值函数 
    - **电梯 MDP (2022):** 给定“上/下”状态和“等待/移动”动作，在随机策略（概率 0.5）下，使用贝尔曼方程计算 V(Up) 和 V(Down) 。
    - **高斯奖励 MDP (2024):** 在一个 2 状态 MDP 中，奖励服从高斯分布，要求计算特定策略下的 Qπ(s,0) 。


#### **3. 数学推导：回报与方差 (Advanced Derivations)**

**考察形式：** 长篇数学证明/推导 **考察频率：** 中 (2024 出现高分大题)

这是 L1 内容最深入的考察方式。题目不满足于让你背诵贝尔曼方程 V=R+γV，而是要求你从**回报 (Return, Gt​)** 的定义出发，推导更高阶的属性。

- **回报方差的贝尔曼方程 (2024):**
    
    - 题目要求从 Gt​ 的定义出发，推导**回报方差 (Variance of Returns)** σR2​(s) 的递归表达式。这直接考察了对 L1 中贝尔曼方程推导逻辑（即如何将 Gt​ 拆解为 Rt+1​+γGt+1​）的深刻理解 。
    - 题目还可能要求计算特定轨迹下的回报值（MC 方法的基础）。

#### **4. 环境建模 (Modeling Environments)**

**考察形式：** 绘图 / 文本转模型 **考察频率：** 中 (2021)

要求将一段关于实际问题的文字描述转化为标准的 MDP 结构。

- **绘制 MDP 图:**
    
    - **自主水下航行器 (AUV) (2021):** 给定状态（浅/中/深）、动作（泵/不泵）以及相应的转移概率和奖励逻辑，要求画出该任务的 MDP 图示 。这考察了你将 L1 中定义的 S,A,P,R 映射到实际问题的能力。


### **备考建议**

1. **熟练手动解方程组:** 务必练习如何通过手算解决包含 2-3 个未知数的线性方程组（例如 V(s1​)=… 和 V(s2​)=…），这是拿分的基础。
    
2. **推导而非死记:** 不要只背诵 V(s) 的贝尔曼方程。请复习 Lecture 1 中从 Gt​ 开始推导贝尔曼方程的全过程，并尝试自己推导回报的**二阶矩**（即方差），以应对像 2024 年那样的新颖推导题。
    
3. **关注定义细节:** 在做选择题时，注意区分“最优策略”与“最优价值函数”的性质差异，以及折扣因子 γ 的边界情况（如 γ=0 或 γ=1 时的意义）。








***
***
OLD VERSION
### 考点1 马尔可夫性质

**定义**

* 马尔可夫性质的定义与解释

**应用**

* 不同模型对马尔可夫性质的利用/是否使用了马尔可夫性质

### 考点2 折扣因子

**折扣因子与状态价值**

* 折扣因子对于价值函数收敛性的影响

**折扣因子与优化目标**

* 最优策略的目标是获得最大化长期**折损**累计增益

### 考点3 状态/状态空间


### 考点4 动作/动作空间


### 考点5 奖励/奖励函数

**与环境的关系**

* 联合环境/环境转移概率与奖励函数的关系

### 考点6 回报/折损累计增益

**回报的随机性**

* 单个轨迹的回报是具有随机性的

### 考点7 环境转移概率

**联合转移概率**

* 联合 (joint) 环境转移概率 p(s',r|s,a) 能够完整反映环境动态
* 联合环境/环境转移概率与奖励函数的关系

* 随机性在环境转移概率中的影响

### 考点8 策略

**策略与转移概率**

*  区分策略和状态转移概率

**最优策略**

* 最优策略的性质（不一共是确定性策略、不一定唯一）

**策略提升定理**

* 理解策略提升定理，不会出现价值降低，新策略大于等于原策略

**提取最优策略**

* 从状态/动作价值函数中计算策略

**策略与随机性**

* 随机性策略的定义

### 考点9 状态价值函数

**性质**

* 最优价值函数的性质（有且仅有一个）

**定义**

* 状态价值函数的意义（期望），不是一个episode的实际回报

### 考点10 动作价值函数

**计算**

* 会计算指定MDP的动作价值函数

**定义**

*  动作价值Q函数的定义

### 考点11 贝尔曼方程

**计算**

* 指定MDP/MRP贝尔曼期望方程（状态/动作）的计算

**公式**

* 贝尔曼期望方程的迭代式展开（包含多种展开方式）

**扩展**

* r和γ都为1时，贝尔曼状态价值变为到达终点的期望步数（计算）
* 方差的贝尔曼方程（推导/计算步数的方差）

**性质**

* 贝尔曼方程与最优子结构

### 考点12 最优性

**定义**

* 贝尔曼最优性原理、贝尔曼最优性的定义
* 贝尔曼最优方程和贝尔曼方程的区别

**公式**

* 贝尔曼最优方程的多种表示方式
* 最优价值函数与最优动作函数的相等以及这种相等关系的表示公式

### 考点13 马尔可夫决策过程

**基础概念**

*  马尔可夫过程、马尔可夫奖励过程、马尔可夫决策过程、隐马尔科夫模型(HMM)、部分可观测马尔可夫决策过程(POMDF)的基础理解
* MDP与MRP的区分

**目标**

* 马尔可夫决策过程的目标

**绘图**

* 会计算所有需要的值并绘图