
### **1. 算法识别与更新规则辨析 (Algorithm Identification)**

**考察内容：**

- **区分不同的更新公式：** 考试经常列出四个不同的更新公式，要求识别哪一个是动态规划（DP）更新，哪一个是时序差分（TD）或蒙特卡洛（MC）。
    
- **DP 的特征：** 需要识别出 DP 更新是基于**模型 (Model-based)** 的（即使用了 P(s′∣s,a) 和 R），并且使用了**自举 (Bootstrapping)**（即用 V(s′) 更新 V(s)）。
    
- **区分 PI 和 VI：**
    
    - **策略迭代 (PI):** 包含完整的策略评估（直到收敛）和策略改进。
        
    - **价值迭代 (VI):** 只有一步策略评估（或截断的评估）紧接着贪婪改进，且使用 `max` 操作符。
        

**考察形式：**

- **多项选择题 (MCQ):**
    
    - **2021-2022 Q1a:** 给出四个公式，问哪一个是动态规划更新？（答案通常包含求和符号 ∑ 和转移概率 P）。
        
    - **2020-2021 Q1b:** 给出一段伪代码，问它实现了什么算法（如 Value Iteration 或 Policy Evaluation）。


### **2.手动执行价值迭代 (Manual Execution of Value Iteration)**

**考察内容：**

- **计算能力：** 在给定的具体 MDP（通常是 3 个状态左右，如“浅水、中水、深水”）中，手动执行价值迭代算法。
    
- **具体步骤：**
    
    1. 初始化 V0​(s)=0 。
    2. 应用贝尔曼最优方程：Vk+1​(s)=maxa​[R+γ∑P(…)Vk​(s′)] 。
    3. 填写迭代表格（Iteration 1, 2, 3...）。

**考察形式：**

- **计算大题 (Calculation Table):**
    
    - **2021-2022 Q2b:** 这是一个非常经典的考题。题目描述了一个 AUV（水下机器人）的 MDP，要求“应用三次价值迭代 (Value Iteration)，初始值为 0，折扣因子 0.8”，并填写每一次迭代后各状态的价值表。
        
    - **注意：** 这类题目要求必须写出计算公式和过程，不能只写结果。

### **3. 动态规划的理论性质 (Theoretical Properties)**

**考察内容：**

- **模型依赖性：** DP 需要环境的完美模型（转移概率 P 和 奖励 R），这是它的主要限制 。
	
- **同步 vs 异步更新 (Synchronous vs Asynchronous):**
	- **同步:** 需要两个数组（旧值和新值）。
	- **异步 (In-place):** 只需要一个数组，通常收敛更快 。
	- **考点:** 2023-2024 Q1a 考察了异步更新是否通常比同步更新快（是）。
		
- **收敛性 (Convergence):**
	- 策略迭代保证在有限步内收敛 。
	- 策略改进定理 (Policy Improvement Theorem) 保证新策略至少和旧策略一样好 。
	- **易错点:** 2024-2025 Q1b 考察了“在表格型环境中，改进一个状态的策略是否会导致另一个状态价值下降？”（答案是不会，基于单调提升性质）。

### **4. 广义策略迭代 (GPI) 与 维度灾难**

**考察内容：**

- **GPI 的概念：** 评估（Evaluation）和改进（Improvement）是两个相互作用的过程，几乎所有 RL 算法都遵循这一模式 。
- **维度灾难 (Curse of Dimensionality):** DP 的计算复杂度随状态变量数量指数级增长，这是 DP 无法处理大规模问题的原因 。

**考察形式：**

- **多项选择题 (MCQ):**
    
    - 判断关于 GPI 的描述是否正确。
        
    - **2023-2024 Q1a:** 判断“DP 是 Model-free 的方法吗？”（错，是 Model-based）。


### **备考建议 (针对 Lecture 2)**

1. **死磕公式：** 能够一眼区分贝尔曼期望方程（用于策略评估，无 max）和贝尔曼最优方程（用于价值迭代，有 max）。
2. **练习手算 VI：** 找一个简单的 2-3 状态 MDP（例如 lecture slides 里的 GridWorld 或 21/22 年的考题），手动计算前 3 轮迭代。注意 V(s′) 是来自**上一轮**的值。
3. **理解“自举”(Bootstrapping)：** 明白 DP 是利用后继状态的**估计值**来更新当前状态，而不是像 MC 那样利用实际回报。
4. **概念辨析：** 明确 Policy Iteration 和 Value Iteration 的区别在于“评估步骤是否彻底完成”以及“是否显式地在每一步生成策略”。






***
***
OLD VERSION
### 考点1 有模型强化学习

**区分**

* 知道什么是有模型强化学习，与无模型的差别

### 考点2 动态规划性质

**性质**

* 最优子结构与重复子问题

### 考点3 迭代法

**迭代方法**

* 雅可比迭代法（同步）
* 高斯-赛德尔迭代法 （原地 In-place）

**方法对比**

* 收敛速度

### 考点4 自举（Bootstrapping）

**定义**

* 理解自举是什么
