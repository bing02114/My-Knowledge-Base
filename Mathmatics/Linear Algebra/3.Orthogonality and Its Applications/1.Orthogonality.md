### 1.Orthogonal Vectors and Orthogonal Subspaces
#### 1.1 Orthogonal Vectors

**Definition**:

>"Orthogonal" is another word for "perpendicular". Two vectors are orthogonal if their dot product is zero

**The zero vector:**

>The zero vector is orthogonal to all vectors

#### 1.2 Orthogonal Subspaces and Complements

**Definition of Orthogonal Subspaces:**

>Subspace S is orthogonal to subspace T if every vector in S is orthogonal to every vector in T

**The Fundamental Orthogonality**

>The Row Space is orthogonal to the Nullspace
>
>The Column Space is orthogonal to the Left Nullspace

**Orthogonal Complements**

>The row space and nullspace are not just orthogonal; they are orthogonal complements in Rⁿ.
>
>This means the nullspace contains **all** vectors that are orthogonal to the row space, and their dimensions sum to n: `dim(Row Space) + dim(Nullspace) = r + (n-r) = n`.

#### 1.3 The Matrix AᵀA and Solving Inconsistent Systems

**Motivation**

>When a system Ax=b has no solution (often due to inconsistent data in "tall" rectangular matrices where m > n), we seek the "best possible" solution.

**The Normal Equation**

>The core of this method is to solve the related equation to find this optimal solution
>$$A^{T}Ax=A^{T}b$$

**Properties of ATA**

>* It's always a square matrix
>* It's always a symmetric matrix

**Invertibility of ATA**

- The matrix ATA is not always invertible.
    
- **Key Property:** The nullspace of ATA is identical to the nullspace of A: N(ATA)=N(A).
    
- **Condition for Invertibility:** ATA is invertible if and only if the nullspace of A contains only the zero vector, which means the **columns of A must be linearly independent**.

***
### 2.Projections onto Subspaces

#### 2.1 Projection onto a Line

**The Geometric Problem**

>$$a^{T}(b-p)=0$$

**The Projection Formulae**
![](images/projection.png)

>Since p lies on the line of a, we can write p=xa
>
>The Scalar Projection: $$x=\frac{a^{T}b}{a^{T}a}$$
>
>The Vector Projection: $$p = a\frac{a^{T}b}{a^{T}a}$$

 **The Projection Matrix P**

- We can express the projection as a matrix operation, `p = Pb`.
    
- **The Projection Matrix Formula:** $$P=\frac{aa^T}{a^{T}a}$$​.Note that the numerator is a matrix, while the denominator is a scalar.
    
- **Properties of P:**
    
    - The column space of P is the line spanned by `a`, and its rank is 1.
        
    - P is a symmetric matrix (PT=P).
        
    - Projecting a second time does not change the result (P^2=P).


***
### 3.Orthogonal Matrices and Gram-Schmidt Orthogonalization
#### 3.1 Othonormal Vectors and Metrices

**Orthonormal Vectors**

>**Definition:**
>
>A set of vectors is orthonormal if they satisfy two conditions:
>
>* They are matually orthogonal: 
>
>$$q_{i}^{T}q_j=0~for~i≠j$$
>
>* Each vector has unit length 
>
>$$q^{T}_{i}q_{j}=1$$
>
>**Properties**:
>
>Orthonormal vectors are always linearly independent
>

**Orthonormal and Orthogonal Matrices**

>**Orthonormal Matrix:**
>
>A matrix Q (not necessarily square) whose columns are orthonormal vectors
>
>**Key Property:**
>
>If Q has orthonormal columns, then $$Q^{T}Q=I$$
>**Spatial Meaning - Isometric Embedding**
>
><font color="red">The transformation takes a lower-dimensional space, Rn, and "embeds" it into a higher-dimensional (or same-dimensional) space, Rm. </font>
>
><font color="red">It maps the standard orthonormal basis of Rn (i.e., e1​,e2​,…,en​) to a new set of orthonormal vectors (q1​,q2​,…,qn​) within the space Rm</font>

>**Orthogonal Matrix**
>
>A square matrix with orthonormal columns
>
>**Key Property**
>
>For a square matrix Q, $$Q^{T}Q=I$$ implies that the inverse is the transpose $$Q^{-1}=Q^T$$
>**Examples**
>
>Permutation matrices and rotation matrices are orthogonal matrices
>
>**Spatial Meaning - Isometry Isomorphism**
>
><font color="red">The linear transformation represented by U is an isometry isomorphism, or more intuitively, a **Rigid Body Transformation of the space**.</?font>
>
><font color="red">Rotation / Reflection</font>

**Advantages of Using an Orthonormal Basis**

>**Projection Matrix Simplification**
>
>$$P=A(A^{T}A)^{-1}A^{T}$$
>
>$$P=AA^{T}$$
>if A is square matrix
>
>$$P=I$$

### 3.2 The Gram-Schemidt Process

**Goal**

>To convert a set of **linearly independent vectors** (a basis) into an **orthonormal basis** that spans the same space.

**The Process**

- First Vector
		
	- Choose the first orthogonal vector `A` to be `a`
		
- Second Vector
		
	- Create the second orthogonal vector `B` by subtracting the projection of `b` onto `A` from `b`. This is the error vector component.
		
	- $$B=b-proj_{A}(b)=b-\frac{A^{T}b}{A^TA}A$$
		
- Third Vector
		
	- $$C=c-proj_{A}(c)-proj_{B}(c)$$
		
- Normalize
		
	- Once an orthogonal basis {A, B, C, ...} is found, divide each vector by its length to get the final orthonormal basis {q₁, q₂, q₃, ...}.
		
	- $$q_1=\frac{A}{||A||},q_2=\frac{B}{||B||}$$

***
### 3.QR Decomposition

**Concept**:

- **A:** The original matrix with linearly independent columns.
    
- **Q:** The orthonormal matrix whose columns form an orthonormal basis for C(A).
    
- **R:** An upper triangular matrix that relates the original basis A to the orthonormal basis Q.

**The Structure of R**

>The entries of R are the dot products rij​=qiT·​aj​. R is upper triangular because each new orthonormal vector qk​ is orthogonal to the subspace spanned by the previous original vectors (a1​,...,ak−1​), making the dot products zero for i>j.

**Which Matrices Can Be Decomposed**

><font color="red">**The matrix A must have linearly independent columns**</font>

**Is the Decomposition Unique?**

><font color="red">**Without additional constraints, QR decomposition is not unique**</font>




