***
### 1.Classical Stationary Iterative Methods
#### 1.1 Jacobi Method

>1. To calculate each component of the new approximation vector, it uses only the values from the previous vector
>2. All components can be updated simultaneously

#### 1.2 Gauss-Seidel Method

>1. It uses newly computed components from the current iteration
>2. It generally converges faster than the Jacobi Method

#### 1.3 Successive Over-Relaxation Method

>1. An acceleration technique for the Gauss-Seidel Method
>2. Introducing a relaxation parameter w
>3. Finding the optimal w can dramatically improve the rate of convergence

***
### 2.Convergence Theory of Iterative Methods
#### 2.1 Necessary and Sufficient Condition for Convergence

>1. An iterative method converges to the unique solution of Ax=b for any initial guess x0 if and only if the **spectral radius** of the iteration matrix B **is less than 1**
>2. $$p(B)<1,where~p(B)=max|\lambda|,and~\lambda~are~the~eigenvalues~of~B$$
#### 2.2 Sufficient Conditions for Convergence

>1. **Strictly Diagonally Dominant Matrix**
>	Jacobi and Gauss-Seidel method are guaranteed to converge
>	
>2. **Symmetric Positive Definite Matrix**
>	Gauss-Seidel method and the SOR also converges for any w in the range (0,2)
#### 2.3 Stopping Criteria

> 1. **Maximum Number of Iterations**
> 2. **Residual-based Criteria**
> 	Stop when the residual vector is small enough
> 
> 3. **Solution Change Criteria**
> 	Stop when the change between successive approximations is small enough
