### **1. 策略梯度定理与推导 (Policy Gradient Theorem & Derivation)**

**考察内容：**

- **Log-Derivative Trick (对数导数技巧):** 这是策略梯度的核心，即 $\nabla \pi = \pi \nabla \ln \pi$。考试经常要求在具体设定下应用这一定理。
    
- **定理意义：** 理解为什么策略梯度不需要对环境动力学（状态分布）求导，这是该定理最关键的贡献 1111。
    
- **具体应用：** 在给定的简单策略（如线性策略 $\pi(a|s) = \theta \cdot s$）下，手动推导具体的更新规则。
    

**考察形式：**

- **高分大题 (Derivation):**
    
    - **2023-2024 Q3b:** 这是一个极具代表性的题目。给定线性策略 $\pi(a|s;\theta) = \theta \cdot s$，要求利用策略梯度定理推导参数 $\theta$ 的更新规则。这需要应用 $\nabla \ln \pi = \frac{\nabla \pi}{\pi}$ 并结合给定的轨迹数据进行计算 2。
        
- **简答题:**
    
    - **2021-2022 Q3b:** 解释策略梯度解决了什么“复杂性”问题（即无法对未知环境动力学求导），以及策略梯度定理如何提供解决方案（将梯度重写为只与策略和动作价值有关的期望形式）3。
        

### **2. REINFORCE 算法与基线 (REINFORCE & Baselines)**

**考察内容：**

- **高方差问题 (High Variance):** REINFORCE 是基于蒙特卡洛的，因此方差很大。
    
- **基线 (Baseline):** 引入基线（如 $V(s)$）可以显著降低方差，且**不会引入偏差** 4。
    
- **更新方向：** 理解 REINFORCE 的直观含义——如果回报 $G_t$ 为正，则增加该动作的概率；反之则减少 5。
    

**考察形式：**

- **简答/分析题:**
    
    - **2024-2025 Q3c:**
        
        - (i) 更新 $\theta$ 是为了最大化还是最小化 $J(\theta)$？（最大化，因为 $J$ 是期望回报）。
            
        - (ii) 如何在不知道转移概率的情况下估计梯度？（使用样本轨迹的 Log-likelihood）。
            
        - (iii) 解释 REINFORCE 为什么方差大，以及列举两种降低方差的方法（基线、Actor-Critic）并解释原理 6。
            
    - **2020-2021 Q1g:** 在一个连续控制任务中，如果将基线 $X$ 从 0 改为 $f_\theta(s_k)$（即引入状态价值基线），对参数更新有什么影响？（答案：减少回报的方差）。
        

### **3. Actor-Critic 方法 (Actor-Critic Methods)**

**考察内容：**

- **结合值函数：** Actor-Critic 使用 Critic（值函数）来引导 Actor（策略）更新，通过自举（Bootstrapping）引入偏差但显著降低方差 7777。
    
- **Advantage Function (优势函数):** 理解 $A(s,a) = Q(s,a) - V(s)$ 的作用。
    

**考察形式：**

- **多项选择题:**
    
    - **2021-2022 Q1g:** 考察优势函数 $A(s,a)$ 的多种定义形式（如 $Q(s,a) - V(s)$ 或 $r + \gamma V(s') - V(s)$）以及它如何减少方差 8。
        
    - **2022-2023 Q1g:** 判断“Actor-Critic 方法实现了广义策略迭代 (GPI)”是否正确（是）9。
        

### **4. 深度策略梯度算法 (DDPG, PPO 等)**

**考察内容：**

- **DDPG:** 适用于连续动作空间，结合了 DQN 的经验回放和目标网络，本质上是确定性策略梯度 (DPG) 的深度版本 10。
    
- **连续动作：** 策略梯度天然适合连续动作空间（如机器人控制），而基于值的 DQN 难以处理 11111111。
    

**考察形式：**

- **多项选择题:**
    
    - **2021-2022 Q1f:** 选择描述 DDPG 的正确组合：Model-free, Off-policy, Continuous action, Actor-critic 12。
        
    - **2020-2021 Q1f:** 对于一个控制无人机（3个旋转速度，1个线性速度）的任务，确定性策略网络的输出节点数是多少？（答案：4，对应动作维度）。
        
    - **2024-2025 Q1g:** 关于 DDPG 中探索噪声的问题（如果噪声方差为 0 会怎样？策略可能陷入局部最优，无法有效探索）13。
        

### **备考建议 (针对 Lecture 6)**

1. **推导必练：** 必须亲手推导一遍 **Log-Derivative Trick**，并能将其应用到简单的线性策略函数上（参考 23/24 年考题）。这是策略梯度最核心的数学基础。
    
2. **理解 Variance Reduction:** 深刻理解为什么 REINFORCE 方差大（单次 MC 采样随机性强），以及为什么减去 Baseline 不改变期望但能减小方差（数学证明：$\sum b(s) \nabla \pi = 0$）。
    
3. **区分算法特性：**
    
    - **REINFORCE:** MC, High Variance, Unbiased.
        
    - **Actor-Critic:** TD (Bootstrapping), Lower Variance, Biased.
        
    - **DDPG:** Continuous Action, Off-policy (Replay Buffer).
        
4. **连续动作空间:** 记住策略梯度相比于 DQN 的最大优势在于能自然处理连续动作空间（输出高斯分布的均值和方差，或直接输出确定性动作）。