**他的观点**

氪石是通用函数近似定理的反例

**他的证明**

* 他是怎么证明通用函数近似定理是错误的
  * 使用了基于多项式基扩展的逻辑回归无效
  * 使用了GPT2进行特征提取的逻辑回归无效

**他的未来工作**

* 不同的机器学习模型
  * 他也说了需要非线性决策平面
* 不同的学习器
  * 影响不大，在实验中体现
* 收敛性分析
  * loss曲线，在实验中体现
* 函数近似
  * 他也讲出基函数的扩展存在局限性
    * 这里可以跟SVM(RBF)进行对比，尝试扩展到无限维
* 预测不确定性
  * 噪声分析
  * 自信程度分析，在实验中体现

**我的反驳逻辑**

* 证明他的证明方法是存在问题的

  * 基于基扩展的对数逻辑回归
    1. 基扩展是非常局限的
    2. 多项式基扩展带来极大的维度，导致训练数据不够
    3. 采用基扩展的对数逻辑回归仍然是参数的线性模型
    4. 反映出数据集的真实边界显然不是一个简单的多项式
  * 基于GPT-2的对数逻辑回归

* 辅助证明（证明数据集的特征）

  * 采用RBF核的SVM
    1. 可以将数据扩展到无限维，辅助证明基扩展的局限性
    2. 核函数基于欧氏距离，带来严重的维度灾难
    3. 辅助证明哪怕是扩展到无限维，基于基扩展的对数逻辑回归也有问题，对数逻辑回归需要的数据量不够
  * XGBoost
    1. 决策屏幕平行于坐标轴，导致严重的欠拟合
    2. 辅助证明基于树的方法有限，证明数据集的边界是“倾斜”的

* 有效模型

  * 单隐藏层MLP
    1. MLP不进行固定的基扩展，而是学习模型的权重进行变换，不会导致特征数量剧增 
    2. MLP不计算距离，不受维度灾难影响
    3. MLP学习可旋转的决策平面，不受XGBoost的轴对齐数据限制
    4. MLP的每一个神经元经过非线性激活函数得到一个非线性的决策平面，输出为多个可学习的非线性决策平面，构建一个复杂的决策曲面（通用逼近定理）
    5. 通过实验证明MLP有效即可证明通用逼近定理有效

* 实验设置

  * 信息介绍
    * 实验设置介绍
    * 数据集介绍
    * 参数选择方法介绍，网格搜索+K折分析
  * 性能实验
    * 多项式扩展逻辑回归-性能差
    * SVM-高维度性能差
    * XGBoost-性能差
    * MLP-性能完美

  <img src="D:\desktop\work\Imperial\课程\Mathematices-for-Machine-Learning\CourseWork\img\kryptonite_model_comparison.png" alt="kryptonite_model_comparison" style="zoom:80%;" />

  * 学习器/收敛性分析

    * 学习器-只分析MLP
      * SGD
      * Adam

    * 收敛性-只分析MLP
      * loss曲线

  <img src="D:\desktop\work\Imperial\课程\Mathematices-for-Machine-Learning\CourseWork\img\mlp_convergence_comparison.png" alt="mlp_convergence_comparison" style="zoom:60%;" />

  ![mlp_learning_rate_comparison](D:\desktop\work\Imperial\课程\Mathematices-for-Machine-Learning\CourseWork\img\mlp_learning_rate_comparison.png)

  * 自信程度分析

  ![model_confidence_comparison](D:\desktop\work\Imperial\课程\Mathematices-for-Machine-Learning\CourseWork\img\model_confidence_comparison.png)

  * 计算成本分析