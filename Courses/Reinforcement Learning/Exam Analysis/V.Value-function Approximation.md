### **1. DQN 及其关键技术 (Deep Q-Networks & Innovations)**

**考察内容：**

- **核心创新点：** 考试反复考察 DQN 成功的三个关键技术：**经验回放 (Experience Replay)**、**目标网络 (Target Networks)** 和 **奖励截断 (Reward Clipping)**。
    
    - **经验回放：** 打破数据的时间相关性（Correlation），使数据更接近独立同分布 (i.i.d.)，提高数据利用率。
        
    - **目标网络：** 解决“移动目标”(Moving Target) 问题，通过固定目标网络的参数一段时间来稳定训练。
        
- **算法性质：** DQN 是 Off-policy 的（因为使用了 Replay Buffer 中的历史数据），且是 Model-free 的。
    

**考察形式：**

- **多项选择题 (MCQ):**
    
    - **2023-2024 Q1d:** 问“在 Deep RL (如 DQN) 中，使用经验回放的好处是什么？”（答案：打破相关性、数据复用等)。
        
    - **2022-2023 Q1h:** 判断关于 DQN 的陈述正误（如“DQN 是 Off-policy”、“目标网络通常只在每一集结束时更新”（错，是每 C 步更新））。
        
- **简答/分析题:**
    
    - **2021-2022 Q3c:** 这是一个极具深度的题目。题目引用了一个观点：“增加输入帧数 $k$ 和增加 Replay Buffer 大小 $L$ 是同一枚硬币的两面，都是增加信息量”。要求你判断同意与否并解释。
        
        - _解析：_ 不同意。增加 $k$ 是为了解决部分可观测性（让状态更马尔可夫化），而增加 $L$ 是为了打破相关性和稳定训练分布。两者的目的和机制完全不同 。
            
    - **2021-2022 Q3a:** 比较“原始 DQN 架构”（输出所有动作的 Q 值）与“朴素 DQN 架构”（输入状态和动作，输出一个 Q 值）。原始架构更高效，因为一次前向传播就能获得所有动作的 Q 值，便于计算 $\max Q$ 。
        

### **2. 函数近似与 SGD 基础 (Function Approximation & SGD)**

**考察内容：**

- **半梯度 (Semi-gradient):** 理解为什么 TD(0) 的更新被称为“半梯度”方法（因为在求导时，目标值 $R + \gamma \hat{V}(S', \theta)$ 被视为常数，忽略了它对 $\theta$ 的依赖）7。
    
- **线性方法 (Linear Methods):**
    
    - 理解线性近似的形式 $\hat{V}(s, \theta) = \theta^T x(s)$。
        
    - 知道其梯度就是特征向量 $x(s)$。
        

**考察形式：**

- **多项选择题:**
    
    - **2021-2022 Q1d:** 考察线性方法的性质，如“线性方法通过参数 $w$ 和特征向量的内积来近似价值函数”（对）、“线性方法不使用基函数”（错，特征就是基函数）8。
        
    - **2023-2024 Q1b:** 考察线性函数近似的更新公式 $\Delta w = \alpha (Target - \hat{V}) x(s)$ 9。
        

### **3. 特征构建方法 (Feature Construction)**

**考察内容：**

- **Tile Coding (瓦片编码):** 一种稀疏编码方式，通过多个重叠的网格覆盖状态空间，提供局部泛化能力 10。
    
- **Radial Basis Functions (RBFs, 径向基函数):** 也是一种特征构建方法，通常使用高斯函数。Coarse coding 是 RBF 对连续特征的推广。
    

**考察形式：**

- **多项选择题:**
    
    - **2021-2022 Q1d:** 判断“Coarse coding 是 RBF 对连续值特征的推广”是否正确（是）11。
        

### **4. 表格法 vs 近似法 (Tabular vs Approximation)**

**考察内容：**

- **内存与泛化：** 表格法无法处理连续状态或巨大的状态空间，且没有泛化能力（学到一个状态对另一个状态无帮助）。近似法（如神经网络）解决了这些问题。
    

**考察形式：**

- **计算/比较题:**
    
    - **2024-2025 Q3b:** 一个非常具体的计算题。比较算法 1（离散化表格法）和算法 2（线性神经网络）。要求计算当两者的内存占用相等时，离散化的份数 $N$ 是多少。这考察了对参数数量的理解（表格是 $N$ 个状态 $\times$ 动作数，线性网络是 特征维数 $\times$ 动作数）12。
        
- **简答题:**
    
    - **2024-2025 Q3a:** 列举并解释三个理由，说明为什么在 RL 中有时首选函数近似而不是查找表（理由包括：内存限制、连续状态空间、泛化能力）13。
        

### **备考建议 (针对 Lecture 5)**

1. **深入理解 DQN:** 不要只背 DQN 的伪代码，要理解其背后的 _Why_。为什么需要 Target Network？（解耦目标与参数）。为什么需要 Experience Replay？（打破相关性）。这在简答题中是高频考点。
    
2. **线性更新公式:** 记住线性函数近似下的 SGD 更新规则，特别是梯度 $\nabla \hat{V} = x(s)$ 这一项，这在选择题中经常用来混淆视听。
    
3. **关注架构图:** 复习 Lecture Slide 中 DQN 的架构图（输入是图像，输出是所有动作的 Q 值），并理解这种设计相比于 (State, Action) -> Q 的优势（计算效率）。
    
4. **半梯度概念:** 能够用一句话解释清楚什么是 Semi-gradient（求导时忽略目标项对参数的依赖），以及为什么这样做（稳定、简单）。














***
***
OLD VERSION
### 考点1 函数近似

**意义**

* 选择使用函数近似的原因

**方法**

* 函数近似的方法包括哪些

**更新公式**

* 蒙特卡洛和TD(0)的更新公式

**最优解**

* 是否能收敛到全局最优解取决于选取的近似方法

### 考点2 DQN

**经验回放**

* 经验回放缓冲池的意义
	* 启用批量学习，更稳定的收敛
	* 混合来自不同时间步数据来减少数据的非平稳性
	* 防止立即忘记罕见或不常见的经验
	* 通过从每个经验中学习多次来提高数据效率
	* 减少连续学习样本之间的相关性

**参数量**

* 会计算DQN参数量与Q-Learning等表格方法的参数量对比

**DQN与卷积神经网络**

* 处理高维数据时使用卷积神经网络


