### 1.Joint Distribution Function

>For a random vector (X,Y), the function defined as
>
>$$F(x, y) = P(X \le x, Y \le y)$$
>
>is called the joint distribution funtion of (X,Y)
>
>**The joint distribution function is non-decreasing with respect to both x and y**

### 2.Marginal Distribution Function

>When F(x,y) is the joint distribution function of (X,Y), since {Y≤∞} and {X≤∞} are certain events, for X and Y we have marginal distribution functions:
>
>$$F_X(x) = P(X \le x, Y \le \infty) = F(x, \infty)$$
>
>$$F_Y(y) = P(X \le \infty, Y \le y) = F(\infty, y)$$
>
>here, these two functions are called the marginal distribution functions of (X,Y)

### 3.Independence

>The necessary and sufficient condition for X,Y to be mutually independent is that for any x,y, the equation 
>
>$$F(x, y) = F_X(x)F_Y(y)$$
>
>holds. This principle can be extended to any number of random variables